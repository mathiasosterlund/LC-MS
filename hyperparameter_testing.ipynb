{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd069092",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import scipy.io\n",
    "from scipy.sparse import coo_matrix\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from sys import getsizeof\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from MyDataset import MyDataset\n",
    "import random\n",
    "\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70f03cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)\n",
    "torch.manual_seed(10) # set seed before creating model\n",
    "\n",
    "images = torch.load('256x256_images_50_percent.pt')\n",
    "labels = torch.load('256x256_labels_50_percent.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc5a36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of epochs to train the model\n",
    "##n_epochs = 30\n",
    "# Learning rate of optimizer\n",
    "##learning_rate = 0.0001\n",
    "# Batch size of data loaders and batch size used when training model\n",
    "batch_size = 64\n",
    "#dropout rate\n",
    "##dropout = 0.30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea010dd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(images))\n",
    "print(len(labels))\n",
    "\n",
    "random.Random(10).shuffle(images) # shuffling with seed\n",
    "random.Random(10).shuffle(labels) \n",
    "\n",
    "size = len(images)\n",
    "\n",
    "dataset = MyDataset(images,labels)\n",
    "\n",
    "split_indices = list(range(0,size))\n",
    "\n",
    "train_idx=split_indices[0:round(0.70*size)]\n",
    "val_idx=split_indices[round(0.70*size):round(0.85*size)]\n",
    "test_idx=split_indices[round(0.85*size):]\n",
    "print(train_idx)\n",
    "print(val_idx)\n",
    "print(test_idx)\n",
    "\n",
    "train_dataset=MyDataset([images[i] for i in train_idx],[labels[i] for i in train_idx])\n",
    "val_dataset=MyDataset([images[i] for i in val_idx],[labels[i] for i in val_idx])\n",
    "test_dataset=MyDataset([images[i] for i in test_idx],[labels[i] for i in test_idx])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n",
    "valid_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "for data, target in train_loader:\n",
    "    print(data.shape)\n",
    "    print(target.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f018e974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the CNN architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, layerAmount, pooling_stride, dropout, startFilter):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv=nn.ModuleList()\n",
    "        self.conv_bn=nn.ModuleList()\n",
    "        \n",
    "        self.layerAmount=layerAmount\n",
    "        \n",
    "        self.kernel_size = 3\n",
    "        self.padding = 1\n",
    "        self.stride = 1\n",
    "\n",
    "\n",
    "        self.conv.append(nn.Conv2d(1, startFilter, self.kernel_size, 1, self.padding))\n",
    "        self.conv_bn.append(nn.BatchNorm2d(startFilter))\n",
    "        \n",
    "        #The layer computation here might not be 100% correct but it atleast calculates correctly for 3-6 layers\n",
    "        for i in range(1, self.layerAmount):\n",
    "            self.conv.append(nn.Conv2d(startFilter*(2**(i-1)), startFilter*(2**i), self.kernel_size, 1, self.padding))\n",
    "            self.conv_bn.append(nn.BatchNorm2d(startFilter*(2**i)))\n",
    "            \n",
    "        x=256\n",
    "        y=256\n",
    "        d=startFilter*(2**(self.layerAmount-1))\n",
    "        \n",
    "        for i in range(0, self.layerAmount):\n",
    "            x=(x-self.kernel_size+(2*self.padding))/pooling_stride[0]\n",
    "            y=(y-self.kernel_size+(2*self.padding))/pooling_stride[1]\n",
    "            #print(math.ceil(x))\n",
    "            #print(math.ceil(y))\n",
    "            #print('')\n",
    "            \n",
    "        ##\n",
    "        \n",
    "            \n",
    "        self.features = math.ceil(x) * math.ceil(y) * math.ceil(d)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=pooling_stride)\n",
    "        \n",
    "        # linear layer (X -> 10)\n",
    "        self.fc1 = nn.Linear(self.features, 10)\n",
    "        # linear layer (10 -> 1)\n",
    "        self.fc2 = nn.Linear(10, 1)\n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        for i in range(0, self.layerAmount):\n",
    "            x = self.conv[i](x) #convolution\n",
    "            x = self.conv_bn[i](x) # batch normalization\n",
    "            x = F.relu(x) # reLU\n",
    "            x = self.pool(x) # pooling\n",
    "            #print(x.shape)\n",
    "        \n",
    "        x = x.view(-1, self.features)\n",
    "        #print(x.shape)\n",
    "        x = self.dropout(x)\n",
    "        # add 1st hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #print(x.shape)\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # output\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        x = x.view(x.size(0))\n",
    "        #print(x.shape)    \n",
    "        return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a1c00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "def objective(trial):\n",
    "    n_epochs = trial.suggest_int(\"n_epochs\", 30, 70)\n",
    "    dropout_p = trial.suggest_float(\"dropout_p\", 0.0, 0.7)\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 3, 6)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-6, 1e-2, log=True)\n",
    "    #x_stride = trial.suggest_int(\"x_stride\", 2, 3)\n",
    "    #y_stride = trial.suggest_int(\"y_stride\", 2, 3)\n",
    "    \n",
    "    model = Net(layerAmount=n_layers, pooling_stride=[2, 2], dropout=dropout_p, startFilter=8)\n",
    "    \n",
    "    # specify loss function (Binary cross entropy)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # specify optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    if not train_on_gpu:\n",
    "        print('CUDA is not available.  Training on CPU ...')\n",
    "    else:\n",
    "        model.cuda()\n",
    "        criterion.cuda()\n",
    "        print('CUDA is available!  Training on GPU ...')\n",
    "        \n",
    "    valid_loss_min = np.Inf # track change in validation loss\n",
    "\n",
    "    train_loss= [0.0] * n_epochs\n",
    "    valid_loss= [0.0] * n_epochs\n",
    "\n",
    "    max_accuracy = 0\n",
    "    \n",
    "    for epoch in range(0, n_epochs):\n",
    "\n",
    "        # keep track of training and validation loss\n",
    "        #train_loss[epoch] = 0.0\n",
    "        #valid_loss[epoch] = 0.0\n",
    "\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        for data, target in train_loader:\n",
    "            data=data.to_dense() # model needs dense matrices as input\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            if train_on_gpu:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the batch loss\n",
    "            output = output.to(torch.float64) #\n",
    "            target = target.to(torch.float64) #\n",
    "            loss = criterion(output, target)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update training loss\n",
    "            train_loss[epoch] += loss.item()*data.size(0)\n",
    "\n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################     \n",
    "        total = 0\n",
    "        true_positive = 0\n",
    "        true_negative = 0\n",
    "        \n",
    "        model.eval()\n",
    "        for data, target in valid_loader:\n",
    "            data=data.to_dense() # model needs dense matrices as input\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            if train_on_gpu:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the batch loss\n",
    "            output = output.to(torch.float64) #\n",
    "            target = target.to(torch.float64) #\n",
    "            #print(output)\n",
    "            #print(target)\n",
    "            loss = criterion(output, target)\n",
    "            # update average validation loss \n",
    "            valid_loss[epoch] += loss.item()*data.size(0)\n",
    "            \n",
    "            for i in range(len(output)):\n",
    "                if (target[i] == 1) and (output[i] >= 0.5):\n",
    "                    true_positive += 1\n",
    "                elif (target[i] == 0) and (output[i] < 0.5):\n",
    "                    true_negative += 1\n",
    "                total +=1\n",
    "\n",
    "        # calculate average losses\n",
    "        train_loss[epoch] = train_loss[epoch]/len(train_loader.sampler)\n",
    "        valid_loss[epoch] = valid_loss[epoch]/len(valid_loader.sampler)\n",
    "\n",
    "        accuracy = (true_positive + true_negative)/total\n",
    "        \n",
    "        if (accuracy > max_accuracy):\n",
    "            max_accuracy = accuracy\n",
    "            \n",
    "        print('accuracy:' + str(accuracy) + ', best epoch:' + str(max_accuracy))\n",
    "        \n",
    "        trial.report(max_accuracy, epoch+1)\n",
    "        \n",
    "        # Handle pruning based on the intermediate value.\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "            \n",
    "    return max_accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a3a81e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=200, timeout=None, show_progress_bar=True)\n",
    "\n",
    "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1285c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bffb5e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046a77fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7823c03f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc9dc4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d512273",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
