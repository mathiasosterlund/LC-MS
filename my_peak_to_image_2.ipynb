{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd069092",
   "metadata": {},
   "outputs": [],
   "source": [
    "#U-Net for prediction of peak positions\n",
    "\n",
    "import scipy.io\n",
    "from scipy.sparse import coo_matrix\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from sys import getsizeof\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from MyDataset import MyDataset\n",
    "import random\n",
    "\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70f03cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "images = torch.load('256x256_images_100_percent.pt')\n",
    "labels = torch.load('256x256_masks_100_percent.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc5a36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 100\n",
    "# Learning rate of optimizer\n",
    "learning_rate = 0.01\n",
    "# Batch size of data loaders and batch size used when training model\n",
    "batch_size = 48\n",
    "#dropout rate\n",
    "dropout = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea010dd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(images))\n",
    "print(len(labels))\n",
    "\n",
    "random.Random(10).shuffle(images) # shuffling with seed\n",
    "random.Random(10).shuffle(labels) \n",
    "\n",
    "#images=images[0:500]\n",
    "#labels=labels[0:500]\n",
    "\n",
    "size = len(images)\n",
    "\n",
    "dataset = MyDataset(images,labels)\n",
    "\n",
    "split_indices = list(range(0,size))\n",
    "\n",
    "train_idx=split_indices[0:round(0.70*size)]\n",
    "val_idx=split_indices[round(0.70*size):round(0.85*size)]\n",
    "test_idx=split_indices[round(0.85*size):]\n",
    "print(train_idx)\n",
    "print(val_idx)\n",
    "print(test_idx)\n",
    "\n",
    "train_dataset=MyDataset([images[i] for i in train_idx],[labels[i] for i in train_idx])\n",
    "val_dataset=MyDataset([images[i] for i in val_idx],[labels[i] for i in val_idx])\n",
    "test_dataset=MyDataset([images[i] for i in test_idx],[labels[i] for i in test_idx])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n",
    "valid_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1)\n",
    "\n",
    "for data, target in train_loader:\n",
    "    print(data.shape)\n",
    "    print(target.shape)\n",
    "    \n",
    "for data, target in test_loader:\n",
    "    print(data.shape)\n",
    "    print(target.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5bdc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the CNN architecture\n",
    "\n",
    "class conv_block(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_c, out_c, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_c)\n",
    "        self.conv2 = nn.Conv2d(out_c, out_c, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_c)\n",
    "        self.relu = nn.ReLU()\n",
    "            \n",
    "    def forward(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "class encoder_block(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "        self.conv = conv_block(in_c, out_c)\n",
    "        self.pool = nn.MaxPool2d((2, 2))\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv(inputs)\n",
    "        p = self.pool(x)\n",
    "        return x, p\n",
    "    \n",
    "class decoder_block(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_c, out_c, kernel_size=2, stride=2, padding=0)\n",
    "        self.conv = conv_block(out_c+out_c, out_c)\n",
    "    def forward(self, inputs, skip):\n",
    "        x = self.up(inputs)\n",
    "        x = torch.cat([x, skip], axis=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class unet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \"\"\" Encoder \"\"\"\n",
    "        self.e1 = encoder_block(1, 16)\n",
    "        self.e2 = encoder_block(16, 32)\n",
    "        self.e3 = encoder_block(32, 64)\n",
    "        self.e4 = encoder_block(64, 128)\n",
    "        \"\"\" Bottleneck \"\"\"\n",
    "        self.b = conv_block(128, 256)\n",
    "        \"\"\" Decoder \"\"\"\n",
    "        self.d1 = decoder_block(256, 128)\n",
    "        self.d2 = decoder_block(128, 64)\n",
    "        self.d3 = decoder_block(64, 32)\n",
    "        self.d4 = decoder_block(32, 16)\n",
    "        \"\"\" Classifier \"\"\"\n",
    "        self.outputs = nn.Conv2d(16, 1, kernel_size=1, padding=0)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \"\"\" Encoder \"\"\"\n",
    "        #print('inputs:' + str(inputs.shape))\n",
    "        s1, p1 = self.e1(inputs)\n",
    "        #print('s1:' + str(s1.shape))\n",
    "        #print('p1:' + str(p1.shape))\n",
    "        s2, p2 = self.e2(p1)\n",
    "        #print('s2:' + str(s2.shape))\n",
    "        #print('p2:' + str(p2.shape))\n",
    "        s3, p3 = self.e3(p2)\n",
    "        #print('s3:' + str(s3.shape))\n",
    "        #print('p3:' + str(p3.shape))\n",
    "        s4, p4 = self.e4(p3)\n",
    "        #print('s4:' + str(s4.shape))\n",
    "        #print('p4:' + str(p4.shape))\n",
    "        \"\"\" Bottleneck \"\"\"\n",
    "        b = self.b(p4)\n",
    "        #print('b:' + str(b.shape))\n",
    "        \"\"\" Decoder \"\"\"\n",
    "        d1 = self.d1(b, s4)\n",
    "        #print('d1:' + str(d1.shape))\n",
    "        d2 = self.d2(d1, s3)\n",
    "        #print('d2:' + str(d2.shape))\n",
    "        d3 = self.d3(d2, s2)\n",
    "        #print('d3:' + str(d3.shape))\n",
    "        d4 = self.d4(d3, s1)\n",
    "        #print('d4:' + str(d4.shape))\n",
    "        \"\"\" Classifier \"\"\"\n",
    "        x = self.outputs(d4)\n",
    "        outputs = torch.sigmoid(x)\n",
    "        print('outputs:' + str(outputs.shape))\n",
    "        return outputs        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe63555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a complete CNN\n",
    "torch.manual_seed(10) # set seed before creating model\n",
    "model = unet()\n",
    "        \n",
    "#print(model.state_dict()['fc1.weight'])\n",
    "\n",
    "print(model)\n",
    "    \n",
    "# specify loss function (Binary cross entropy)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# specify optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0e24a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "#train_on_gpu = False\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    model.cuda()\n",
    "    criterion.cuda()\n",
    "    print('CUDA is available!  Training on GPU ...')\n",
    "\n",
    "valid_loss_min = np.Inf # track change in validation loss\n",
    "\n",
    "train_loss= [0.0] * n_epochs\n",
    "valid_loss= [0.0] * n_epochs\n",
    "\n",
    "for epoch in range(0, n_epochs):\n",
    "\n",
    "    # keep track of training and validation loss\n",
    "    #train_loss[epoch] = 0.0\n",
    "    #valid_loss[epoch] = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train()\n",
    "    for data, target in train_loader:\n",
    "        data=data.to_dense() # model needs dense matrices as input\n",
    "        target=target.to_dense()\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        output = output.to(torch.float64) #\n",
    "        target = target.to(torch.float64) #\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss[epoch] += loss.item()*data.size(0)\n",
    "        \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval()\n",
    "    for data, target in valid_loader:\n",
    "        data=data.to_dense() # model needs dense matrices as input\n",
    "        target=target.to_dense() # model needs dense matrices as input\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        \n",
    "        output = output.to(torch.float64) \n",
    "        target = target.to(torch.float64) \n",
    "        \n",
    "        #print(output[0][output[0] >= 0.5])\n",
    "        #print(target[0][target[0] != 0])\n",
    "        #print(output)\n",
    "        #print(target)\n",
    "        #maxOutputs = []\n",
    "        #for i in range(output.shape[0]):\n",
    "        #    maxOutputs.append(output[i].max().cpu().detach().numpy() )\n",
    "        #print(maxOutputs)\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        # update average validation loss \n",
    "        valid_loss[epoch] += loss.item()*data.size(0)\n",
    "    \n",
    "    # calculate average losses\n",
    "    train_loss[epoch] = train_loss[epoch]/len(train_loader.sampler)\n",
    "    valid_loss[epoch] = valid_loss[epoch]/len(valid_loader.sampler)\n",
    "        \n",
    "    # print training/validation statistics\n",
    "    print_train_loss=train_loss[epoch]\n",
    "    print_valid_loss=valid_loss[epoch]\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch+1, print_train_loss, print_valid_loss))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss[epoch] <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        print_valid_loss))\n",
    "        torch.save(model.state_dict(), 'my_unet_model.pt')\n",
    "        valid_loss_min = valid_loss[epoch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5d0e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_dpi(200)\n",
    "plt.ylim(0,1)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "line1 = plt.plot(np.linspace(1, n_epochs, num=n_epochs), train_loss)\n",
    "line2 = plt.plot(np.linspace(1, n_epochs, num=n_epochs), valid_loss)\n",
    "plt.legend([\"train loss\", \"valid loss\"])\n",
    "#plt.xticks(np.arange(0, n_epochs, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59e7d5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model_1 = unet()\n",
    "#model_1.load_state_dict(torch.load('my_unet_model_0.001217_sigmoid_BCE_256f_3906d.pt'))\n",
    "#model_1.eval()\n",
    "\n",
    "model_2 = unet()\n",
    "model_2.load_state_dict(torch.load('my_unet_model_0.000310_sigmoid_MSE_256f_3906d.pt'))\n",
    "model_2.eval()\n",
    "\n",
    "\n",
    "#if train_on_gpu:\n",
    "#model.cuda()\n",
    "\n",
    "#outputList_1 = []\n",
    "outputList_2 = []\n",
    "targetList = []\n",
    "dataList = []\n",
    "\n",
    "\n",
    "for data, target in test_loader:\n",
    "        data=data.to_dense() # model needs dense matrices as input\n",
    "        target=target.to_dense()\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        #if train_on_gpu:\n",
    "        #    data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        #output_1 = model_1(data)\n",
    "        # calculate the batch loss\n",
    "        #output_1 = output_1.to(torch.float64) #\n",
    "        \n",
    "        output_2 = model_2(data)\n",
    "        # calculate the batch loss\n",
    "        output_2 = output_2.to(torch.float64) #\n",
    "        \n",
    "        \n",
    "        target = target.to(torch.float64) #\n",
    "\n",
    "        #output_1 = output_1.detach().numpy()\n",
    "        output_2 = output_2.detach().numpy()\n",
    "        target = target.detach().numpy()\n",
    "        \n",
    "        #print(output.shape)\n",
    "        #print(target.shape)\n",
    "        \n",
    "        #outputList_1.append(output_1)\n",
    "        outputList_2.append(output_2)\n",
    "        targetList.append(target)\n",
    "        dataList.append(data)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f2dcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "imageWithCorrectPeak=0\n",
    "for i in range(len(outputList_2)):\n",
    "    correctPeak=0\n",
    "    myTarget=targetList[i]\n",
    "    myTarget=myTarget.flatten()\n",
    "    myOutput_2=outputList_2[i]\n",
    "    myOutput_2=myOutput_2.flatten()\n",
    "    for j in range(myTarget.size):\n",
    "        if (myTarget[j] == 1):\n",
    "            if (myOutput_2[j] > 0.5):\n",
    "                correctPeak +=1\n",
    "                break\n",
    "    if correctPeak > 0:\n",
    "        imageWithCorrectPeak +=1\n",
    "\n",
    "print(str(imageWithCorrectPeak/len(outputList_2)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4a69f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://machinelearningmastery.com/tour-of-evaluation-metrics-for-imbalanced-classification/\n",
    "truePositive=0\n",
    "falsePositive=0\n",
    "falseNegative=0\n",
    "total=0\n",
    "for i in range(len(outputList_2)):\n",
    "    myTarget=targetList[i]\n",
    "    myTarget=myTarget.flatten()\n",
    "    myOutput_2=outputList_2[i]\n",
    "    myOutput_2=myOutput_2.flatten()\n",
    "    for j in range(myTarget.size):\n",
    "        total +=1\n",
    "        if (myTarget[j] == 1):\n",
    "            if (myOutput_2[j] >= 0.5):\n",
    "                truePositive +=1\n",
    "            else:\n",
    "                falseNegative +=1\n",
    "        elif (myOutput_2[j] >= 0.5):\n",
    "            falsePositive +=1\n",
    "                \n",
    "print(str(truePositive))\n",
    "print(str(falsePositive))\n",
    "print(str(falseNegative))\n",
    "print(str(total))\n",
    "\n",
    "precision = truePositive / (truePositive + falsePositive)\n",
    "recall = truePositive / (truePositive + falseNegative)\n",
    "f1_measure = (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "print(str(precision))\n",
    "print(str(recall))\n",
    "print(str(f1_measure))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa014b53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_nr = 97\n",
    "\n",
    "myTarget=targetList[image_nr]\n",
    "\n",
    "#myOutput_1=outputList_1[image_nr]\n",
    "\n",
    "myOutput_2=outputList_2[image_nr]\n",
    "\n",
    "myData=dataList[image_nr]\n",
    "\n",
    "\n",
    "#print(myTarget[myTarget != 0.5])\n",
    "#print(myOutput_1[myOutput_1 >= 0.5])\n",
    "#print(myOutput_2[myOutput_2 >= 0.5])\n",
    "\n",
    "myTarget=myTarget.flatten()\n",
    "#print(myTarget.shape)\n",
    "\n",
    "#myOutput_1=myOutput_1.flatten()\n",
    "#print(myOutput.shape)\n",
    "\n",
    "myOutput_2=myOutput_2.flatten()\n",
    "#print(myOutput.shape)\n",
    "\n",
    "#cutOff = min(0.5,myOutput_2.max())\n",
    "cutOff = 0.5\n",
    "\n",
    "#myOutput_1[myOutput_1 >= cutOff] = 1\n",
    "#myOutput_1[myOutput_1 < cutOff] = 0\n",
    "myOutput_2[myOutput_2 >= cutOff] = 1\n",
    "myOutput_2[myOutput_2 < cutOff] = 0\n",
    "\n",
    "#peaks=0\n",
    "#correctPeak=0\n",
    "#falsePeak=0\n",
    "#for j in range(myTarget.size):\n",
    "#    if (myTarget[j] == 1):\n",
    "#       peaks += 1\n",
    "#        if (myOutput_1[j] > 0.5):\n",
    "#            correctPeak +=1\n",
    "#    else:\n",
    "#        if (myOutput_1[j] > 0.5):\n",
    "#            falsePeak += 1\n",
    "\n",
    "#print('BCE')\n",
    "#print('peaks:' + str(peaks))\n",
    "#print('correctPeak:' + str(correctPeak))\n",
    "#print('falsePeak:' + str(falsePeak))\n",
    "\n",
    "\n",
    "peaks=0\n",
    "correctPeak=0\n",
    "falsePeak=0\n",
    "for j in range(myTarget.size):\n",
    "    if (myTarget[j] == 1):\n",
    "        peaks += 1\n",
    "        if (myOutput_2[j] > 0.5):\n",
    "            correctPeak +=1\n",
    "    else:\n",
    "        if (myOutput_2[j] > 0.5):\n",
    "            falsePeak += 1\n",
    "\n",
    "print('MSE')\n",
    "print('peaks:' + str(peaks))\n",
    "print('correctPeak:' + str(correctPeak))\n",
    "print('falsePeak:' + str(falsePeak))\n",
    "\n",
    "\n",
    "\n",
    "ls=np.linspace(1,256,256) \n",
    "x_axis=np.tile(ls,256) \n",
    "#print(x_axis.shape)\n",
    "#print(x_axis)\n",
    "\n",
    "ls=np.linspace(1,256,256) \n",
    "y_axis=np.repeat(ls,256) \n",
    "#print(y_axis.shape)\n",
    "#print(y_axis)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_dpi(200)\n",
    "plt.xlabel('x (m/z)')\n",
    "plt.ylabel('y (Retention time)')\n",
    "plt.xlim([-5, 200])\n",
    "plt.ylim([-5, 240])\n",
    "plt.tick_params(axis='x', bottom=False, labelbottom=False)\n",
    "plt.tick_params(axis='y', left=False, labelleft=False)\n",
    "plt.scatter(x_axis, y_axis, s=myTarget)\n",
    "ax.set_title('Target')\n",
    "\n",
    "#fig, ax = plt.subplots()\n",
    "#fig.set_dpi(200)\n",
    "#plt.scatter(x_axis, y_axis, s=myOutput_1)\n",
    "#ax.set_title('Output BCE')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_dpi(200)\n",
    "plt.xlim([-5, 200])\n",
    "plt.ylim([-5, 240])\n",
    "plt.tick_params(axis='x', bottom=False, labelbottom=False)\n",
    "plt.tick_params(axis='y', left=False, labelleft=False)\n",
    "plt.xlabel('x (m/z)')\n",
    "plt.ylabel('y (Retention time)')\n",
    "plt.scatter(x_axis, y_axis, s=myOutput_2)\n",
    "ax.set_title('Output') #MSE\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_dpi(200)\n",
    "plt.xlim([-5, 200])\n",
    "plt.ylim([-5, 240])\n",
    "plt.tick_params(axis='x', bottom=False, labelbottom=False)\n",
    "plt.tick_params(axis='y', left=False, labelleft=False)\n",
    "plt.xlabel('x (m/z)')\n",
    "plt.ylabel('y (Retention time)')\n",
    "plt.scatter(x_axis, y_axis, (0.2*myData)**1)\n",
    "ax.set_title('Intensities')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_dpi(200)\n",
    "plt.xlim([-5, 200])\n",
    "plt.ylim([-5, 240])\n",
    "plt.tick_params(axis='x', bottom=False, labelbottom=False)\n",
    "plt.tick_params(axis='y', left=False, labelleft=False)\n",
    "plt.xlabel('x (m/z)')\n",
    "plt.ylabel('y (Retention time)')\n",
    "plt.scatter(x_axis, y_axis, (0.2*myData)**1)\n",
    "plt.scatter(x_axis, y_axis, s=8*myOutput_2, color='k')\n",
    "ax.set_title('Intensities')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_dpi(200)\n",
    "plt.xlim([-5, 200])\n",
    "plt.ylim([-5, 240])\n",
    "plt.tick_params(axis='x', bottom=False, labelbottom=False)\n",
    "plt.tick_params(axis='y', left=False, labelleft=False)\n",
    "plt.xlabel('x (m/z)')\n",
    "plt.ylabel('y (Retention time)')\n",
    "plt.scatter(x_axis, y_axis, (0.2*myData)**1)\n",
    "plt.scatter(x_axis, y_axis, s=8*myOutput_2, color='k')\n",
    "plt.scatter(x_axis, y_axis, s=1*myTarget, color='r')\n",
    "ax.set_title('Output')\n",
    "              \n",
    "              \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f12b498",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
